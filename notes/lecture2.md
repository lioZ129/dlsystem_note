# 大纲
- 机器学习基础
- softmax回归

# 机器学习算法三要素
- **假设类**：“程序结构”，通过一组参数参数化，描述了我们如何将输入（例如，数字图像）映射到输出（例如，类标签或不同类标签的概率）
- **Loss函数**：指定给定假设（即参数的选择）在所要求的任务上执行得有多“好”的函数
- **优化方法**：确定一组参数的过程，这些参数（近似地）最小化训练集的损失总和

# 多分类问题
## 设置
- n = 输入数据的维度
- k = 不同类别/标签的数量
- m = 训练集中的点数
- x = 维度为n * 1的单训练集输入矩阵
- y = 单训练集的类别/标签（离散的）

## 线性假设函数
### 单训练集
- h(x) = Theta转置 * x
- 目的：将输入（n * 1的矩阵）映射到输出（k * 1的矩阵，其中每一项是与此输入相对应的类的置信度 **非概率，概率为后文softmax的结果**，并非离散）
- 线性过程：为了符合输入输出的维度关系，需要的参数Theta为一个n * k的矩阵
### 多训练集
- m个训练集，每个训练集n维，故引入 X :m * n的矩阵，为整个训练集的输入矩阵（与设置中单训练集输入 x 区分开）
- 同时设置中y也从单训练集的一个值变成m维
- 线性假设函数输出由k * 1变为k * m
- h(X) = X * Theta
> 注意为大写X，之所以不用转置因为大X在定义中将小x转置过了

# Loss函数 
损失越小越好
- 1.**经典错误**：设Loss函数为零一函数 模型预测准确为0（h(x) == y）; otherwise为1
    缺陷：不可微 无法提供损失信息
- 2.**softmax/cross-entropy loss（交叉熵损失）**：z = p(模型预测结果 == 实际训练集结果) = normalize(exp(h(x))) 
> normalize(exp(h(x))) : 将h(x)先求指数确保为正，再标准化为[0，1]范围内的各个类别的预测概率

为满足Loss函数值越小越拟合，设Loss(h(x)预测值,y真实值) = - log(p(label == y))
> 这里的损失函数不是|预测值-真实值|的越小越好而是取-log，我认为softmax的本质是对于多分类问题（其实其他问题也可以看看能不能如此），将线性假设函数的输出通过取幂和标准化变成[0，1]范围内的各个类别的预测概率，接着就可以将损失函数定义为**对真实类别的预测概率的负对数**，契合负对数在(0,1)定义域上自变量越接近1函数值越小的特点

  - 要使Loss函数尽可能小，用梯度下降法（不断朝着最快下降速度的方向迭代Theta）
  - 梯度下降公式：Theta = Theta - 学习率(步长) * f(Theta)对Theta的偏导
    在这里f(Theta)为训练集loss函数求和再取平均
   > Theta为n * k的矩阵，故f(Theta)也为矩阵，对其矩阵内各元素偏导仍为n * k矩阵
  - 如何求梯度下降公式中f(Theta)对Theta的偏导？
    1. 使用矩阵微分学，雅可比矩阵，克罗内克积和向量化（正确但易出问题）
    2. 假设一切都是标量，使用典型的链式法则，然后重新排列/转置矩阵/向量以使得出的偏导矩阵大小匹配答案矩阵大小（需检查数值答案）（大众方法）

# SGD随机批次梯度下降
- 每单个样本都可以得出一个梯度，但单样本梯度下降缺点明显：梯度噪声大，更新方向不稳定，收敛慢
- SGD大意是总样本集取若干数目的小样本集，每个小样本集中的样本矩阵(m*n)分别求其平均梯度，再以一个循环数 = 总样本数/小样本集样本数 的循环梯度下降