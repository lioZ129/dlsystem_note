{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e001eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d2d38",
   "metadata": {},
   "source": [
    "# 单细胞LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e8c83e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 100])\n",
      "torch.Size([400, 20])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个LSTM单元，输入维度20，隐藏层维度100\n",
    "model = nn.LSTMCell(20, 100)\n",
    "\n",
    "# 打印隐藏层到隐藏层的权重矩阵形状\n",
    "# 形状为[4*hidden_size, hidden_size]，对应四个门（输入/遗忘/细胞/输出）的权重\n",
    "print(model.weight_hh.shape)  # 输出: torch.Size([400, 100])\n",
    "\n",
    "# 打印输入层到隐藏层的权重矩阵形状\n",
    "# 形状为[4*hidden_size, input_size]，对应四个门的输入权重\n",
    "print(model.weight_ih.shape)  # 输出: torch.Size([400, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a4c4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# 自定义LSTM单元实现\n",
    "def lstm_cell(x, h, c, W_hh, W_ih, b):\n",
    "    # 合并计算四个门的线性变换: W_ih@x + W_hh@h + b\n",
    "    # 输出形状为[4*hidden_size]，按四个门切割\n",
    "    i, f, g, o = np.split(W_ih @ x + W_hh @ h + b, 4)\n",
    "    \n",
    "    # 应用激活函数\n",
    "    i,f,g,o = sigmoid(i), sigmoid(f), np.tanh(g), sigmoid(o)\n",
    "    \n",
    "    # 更新细胞状态\n",
    "    c_out = f * c + i * g\n",
    "    \n",
    "    # 计算隐藏状态\n",
    "    h_out = o * np.tanh(c_out)\n",
    "    \n",
    "    return h_out, c_out  # 返回新隐藏状态和细胞状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e185e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成随机输入数据（1个样本，20维特征）\n",
    "x = np.random.randn(1, 20).astype(np.float32)\n",
    "h0 = np.random.randn(1, 100).astype(np.float32)  # 初始隐藏状态\n",
    "c0 = np.random.randn(1, 100).astype(np.float32)  # 初始细胞状态\n",
    "\n",
    "# 使用PyTorch的LSTMCell计算输出\n",
    "h_pytorch, c_pytorch = model(\n",
    "    torch.tensor(x), \n",
    "    (torch.tensor(h0), torch.tensor(c0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c26a23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5220228 ,  0.37693432, -1.7825887 , -1.09794   ,  0.24591659,\n",
       "        0.6271068 , -0.6358606 ,  0.6282339 ,  0.4833358 ,  0.40714845,\n",
       "        0.43824404, -0.23232095, -0.67724615,  0.80638236, -0.6009789 ,\n",
       "       -0.05727648,  0.26365104, -1.0171974 , -0.5647117 ,  0.66235757],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # 注意区分和x的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2539a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2367168e-07\n",
      "3.9906277e-07\n"
     ]
    }
   ],
   "source": [
    "# 使用自定义LSTM单元计算结果\n",
    "h_custom, c_custom = lstm_cell(\n",
    "    x[0], h0[0], c0[0],\n",
    "    model.weight_hh.detach().numpy(),     # 获取PyTorch权重并转换为NumPy\n",
    "    model.weight_ih.detach().numpy(),\n",
    "    (model.bias_hh + model.bias_ih).detach().numpy()  # 合并偏置项\n",
    ")\n",
    "\n",
    "# 比较自定义实现与PyTorch结果的差异（L2范数）\n",
    "print(np.linalg.norm(h_pytorch.detach().numpy() - h_custom))  # 误差\n",
    "print(np.linalg.norm(c_pytorch.detach().numpy() - c_custom))  # 误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7db594",
   "metadata": {},
   "source": [
    "# 全序列LSTM\n",
    "- 为了处理长序列的拆分，函数返回所有的hidden state以及最后一个cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce625b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.LSTM(20,100,num_layers=1)  # 生成测试数据（50步序列，20维特征）\n",
    "\n",
    "X_seq = np.random.randn(50, 20).astype(np.float32)\n",
    "h0 = np.random.randn(1, 100).astype(np.float32)  # 初始隐藏状态\n",
    "c0 = np.random.randn(1, 100).astype(np.float32)  # 初始细胞状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ea2e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54967600491765e-06\n"
     ]
    }
   ],
   "source": [
    "# 定义完整LSTM序列处理函数\n",
    "def lstm(X, h, c, W_hh, W_ih, b):\n",
    "    # 初始化输出隐藏状态矩阵 [序列长度, 隐藏维度]\n",
    "    H = np.zeros((X.shape[0], h.shape[0]))\n",
    "    for t in range(X.shape[0]):\n",
    "        # 逐时间步调用LSTM单元\n",
    "        h, c = lstm_cell(X[t], h, c, W_hh, W_ih, b)\n",
    "        H[t, :] = h  # 保存当前隐藏状态\n",
    "    return H, c  # 返回所有隐藏状态和最终细胞状态\n",
    "\n",
    "# 调用自定义LSTM处理序列\n",
    "H_custom, cn_custom = lstm(\n",
    "    X_seq, h0[0], c0[0],\n",
    "    model.weight_hh_l0.detach().numpy(),\n",
    "    model.weight_ih_l0.detach().numpy(),\n",
    "    (model.bias_hh_l0 + model.bias_ih_l0).detach().numpy()\n",
    ")\n",
    "\n",
    "H_pytorch, (hn_pytorch, cn_pytorch) = model(torch.tensor(X_seq)[:,None,:],\n",
    "                                            (torch.tensor(h0)[:,None,:],torch.tensor(c0)[:,None,:]))\n",
    "# [:,None,:]在张量的第二维插入一个大小为1的维度，适配PyTorch LSTM对输入形状必须包含批次维度的要求\n",
    "\n",
    "# 结果对比\n",
    "print(np.linalg.norm(H_custom - H_pytorch[:,0,:].detach().numpy()))  # 误差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6982da21",
   "metadata": {},
   "source": [
    "# 添加批处理的LSTM\n",
    "- 内存连续的矩阵乘法效果最好，而lstm是按时间步来遍历矩阵的，如果按习惯将batch作为第一个维度，即[batch_size,timesteps,imput_size]则访问时为[:,t,:]内存排放不连续，故需要将t放在第一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b1e9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义批处理优化的LSTM单元\n",
    "def lstm_cell(x, h, c, W_hh, W_ih, b):\n",
    "    # 输入x形状: [batch_size, input_size]\n",
    "    # 矩阵乘法优化计算\n",
    "    gates = x @ W_ih + h @ W_hh + b[None, :]  # 广播偏置\n",
    "    i, f, g, o = np.split(gates, 4, axis=1)    # 按列切割\n",
    "    \n",
    "    # 激活函数\n",
    "    i, f, g, o = sigmoid(i), sigmoid(f), np.tanh(g), sigmoid(o)\n",
    "    c_out = f * c + i * g\n",
    "    h_out = o * np.tanh(c_out)\n",
    "    return h_out, c_out\n",
    "\n",
    "# 批处理LSTM实现\n",
    "def lstm(X, h, c, W_hh, W_ih, b):\n",
    "    H = np.zeros((X.shape[0], X.shape[1], h.shape[1]))  # 输出形状[T, B, N]\n",
    "    for t in range(X.shape[0]):\n",
    "        h, c = lstm_cell(X[t], h, c, W_hh, W_ih, b)\n",
    "        H[t, :, :] = h  # 保存当前批的隐藏状态\n",
    "    return H, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60648be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = np.random.randn(50, 128, 20).astype(np.float32)\n",
    "h0 = np.random.randn(1, 128, 100).astype(np.float32)  # 初始隐藏状态\n",
    "c0 = np.random.randn(1, 128, 100).astype(np.float32)  # 初始细胞状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "073265b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1522489051983313e-05\n"
     ]
    }
   ],
   "source": [
    "# 调用自定义LSTM处理序列\n",
    "H_custom, cn_custom = lstm(\n",
    "    X_seq, h0[0], c0[0],\n",
    "    model.weight_hh_l0.detach().numpy().T, # 加.T匹以配前面@运算前后调换了\n",
    "    model.weight_ih_l0.detach().numpy().T,\n",
    "    (model.bias_hh_l0 + model.bias_ih_l0).detach().numpy()\n",
    ")\n",
    "\n",
    "# 官方LSTM\n",
    "H_pytorch, (hn_pytorch, cn_pytorch) = model(torch.tensor(X_seq),\n",
    "                                            (torch.tensor(h0),torch.tensor(c0)))\n",
    "\n",
    "print(np.linalg.norm(H_custom - H_pytorch.detach().numpy()))  # 误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ce0b03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 128, 100])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_pytorch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f68242",
   "metadata": {},
   "source": [
    "# 训练LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4970a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(X, h0, c0, Y, W_hh, W_ih, b, opt):\n",
    "    H, cn = lstm(X, h0, c0, W_hh, W_ih, b)\n",
    "    l = loss(H, Y)\n",
    "    l.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8cec9",
   "metadata": {},
   "source": [
    "- 对于多层多时间步LSTM or RNN,先计算一整条时间步再多层间迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d18b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_lstm(X, h0, c0, Y, W_hh, W_ih, b, opt):\n",
    "    H = X\n",
    "    depth = len(W_hh)\n",
    "    for i in range(depth): # 层间迭代\n",
    "        H, cn = lstm(H, h0[i], c0[i], W_hh[i], W_ih[i], b[i])\n",
    "    l = loss(H, Y)\n",
    "    l.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e263d6",
   "metadata": {},
   "source": [
    "- 当时间步非常长时，需要截断计算图，拆分成许多小部分作正向反向传播更新参数，之间仅传递末尾h/c作为下个部分的h0/c0，故函数要返回末尾hidden/cell\n",
    "- 就是对多层多时间步进行或横向或纵向的计算图细分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "768213dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_lstm(X, h0, c0, Y, W_hh, W_ih, b, opt):\n",
    "    H = X\n",
    "    depth = len(W_hh)\n",
    "    for i in range(depth): # 层间迭代\n",
    "        H, cn = lstm(H, h0[i], c0[i], W_hh[i], W_ih[i], b[i])\n",
    "        h0[i] = H[-1].detech().copy()\n",
    "        c0[i] = cn.detech().copy()\n",
    "\n",
    "    l = loss(H, Y)\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "    return h0,c0\n",
    "\n",
    "# # 训练过程\n",
    "# # 初始化\n",
    "# h0, c0 = np.zeros() \n",
    "# sequence_len, BLOCK_SIZE = ...\n",
    "\n",
    "# for i in range(sequence_len // BLOCK_SIZE):\n",
    "#     h0, c0 = train_deep_lstm(X[i:i+BLOCK_SIZE], h0, c0, Y[i:i+BLOCK_SIZE], W_hh, W_ih, b, opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
